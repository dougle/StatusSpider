#!/bin/bash

usage()
{
cat << EOF
usage: $0 options

This script takes a starting webpage and crawls around spawning processes for every link it finds, then reports back any failiars

OPTIONS:
   -h      Show this message
   -d      delay between calls default 2 seconds
   -i      Ignore this status code e.g. -i 200 (can be used multiple times)
   -l      Location of the log file that will be emailed (default is ./status_spider.log)
   -t      Target URL
   -s      Sub process call, only used to call the child processes for any found links
   -w      Wget options
   
EOF
}


LOG_FILE='./status_spider.log'
HIST_FILE="/tmp/.spider_history"
TARGET=
SUB_PROCESS=1
IGNORE_STATUS=""
DELAY=2
MAX_THREADS=5

while getopts "hd:l:t:sw:b:m:i:q" OPTION
do
     case $OPTION in
         h)
             usage
             exit 1
             ;;
         d)
             DELAY=$OPTARG
             ;;
         l)
             LOG_FILE=$OPTARG
             ;;
         t)
             TARGET=$OPTARG
             ;;
         s)
             SUB_PROCESS=0
             ;;
         w)
         	 WGET_OPTIONS=$OPTARG
         	 ;;
         b)
         	 BASE_URL=$OPTARG
         	 ;;
         m)
           MAX_THREADS=$OPTARG
           ;;
         i)
         	 IGNORE_STATUS="$IGNORE_STATUS|$OPTARG"
         	 ;;
         ?)
             usage
             exit
             ;;
     esac
done

# tidy up ignore status
IGNORE_STATUS=`echo "$IGNORE_STATUS" | sed -r "s/^\|//"`

# default the target to http
if [[ ! "$TARGET" =~ ^[a-z]{3,5}:\/\/ ]];then
	TARGET="http://$TARGET"
fi

if [[ -z $BASE_URL ]];then
	BASE_URL=$TARGET
fi
BASE_URL=`echo "$BASE_URL" | sed -r "s|/$||"` # strip trailing slash

if [[ $SUB_PROCESS -eq 1 ]];then
	echo "" > $HIST_FILE
fi

# create a wget log file
WGET_LOG=`mktemp`
WGET_OUTPUT=`mktemp`

# get target
wget -O $WGET_OUTPUT --no-cache -o $WGET_LOG -k -F -B "$BASE_URL" -S $WGET_OPTIONS "$TARGET"

# log the result of the get
RESULT=`cat "$WGET_LOG" | egrep '^(  HTTP/[0-9\.]+ [0-9]+ [^\n]*|Location: [^\n]*)' | sed -r "s|  HTTP/[0-9\.]+ ([0-9]+ [^\n]*)|\1|"`
if [[ ! "$RESULT" =~ ($IGNORE_STATUS) ]];then
	echo "$TARGET $RESULT" >> "$LOG_FILE"
fi

# exit if we are on an external link
DOMAIN=`echo "$BASE_URL" | sed -r 's|^[a-z]{3,4}://([^/]+)/?.*|\1|'`
if [[ ! "$TARGET" =~ "$DOMAIN" ]];then
	exit 0;
fi

WGET_OUTPUT_FILENAME=`basename "$WGET_OUTPUT"`
LINKS=`pcregrep -N ANY -Mioe '((?<=href=")|(?<=src="))(.+?)(?="( |>|/))' $WGET_OUTPUT | grep -v "$WGET_OUTPUT_FILENAME"`

for LINK in $LINKS
do
	# add on the url to any absolute urls
	LINK=`echo "$LINK" | sed -r "s|^/(.*)$|$BASE_URL/\1|"`

	# default the target to http
	if [[ ! "$LINK" =~ ^[a-z]{3,5}:\/\/ ]];then
		LINK="http://$LINK"
	fi

	# convert &amp; and strip off any anchors
	LINK=`echo "$LINK" | sed -r "s|\&amp;|\&|" | sed -r "s|#.*$||"`

	# check that the link isn't blank after cleaning and that it does reference the current target
	# which will be a temp file after the -k option in wget
	if [[ ! -z "$LINK" ]];then
		# if we haven't been there already
		grep "$LINK" $HIST_FILE
		if [[ $? -eq 1 ]];then
			COMMAND_EXTRAS=""
			if [[ ! -z $IGNORE_STATUS ]];then
				COMMAND_EXTRAS=`echo "$IGNORE_STATUS" | sed -r "s@\|?([0-9]+)\|?@-i \1@"`
			fi
			echo "$LINK" >> $HIST_FILE
			
			sleep $DELAY
			
			while [[ -n `ps -ea | grep status_spider | awk "NR>$MAX_THREADS"` ]];do
			  sleep 1
			done
			
			nohup $0 -t "$LINK" -b $BASE_URL -l $LOG_FILE -s -w "$WGET_OPTIONS" $COMMAND_EXTRAS > /dev/null &
		fi
	fi
done

exit 0
