#!/bin/bash

usage()
{
cat << EOF
usage: $0 options

This script takes a starting webpage and crawls around spawning processes for every link it finds, then reports back any failiars

OPTIONS:
   -h      Show this message
   -d      delay between calls default 2 seconds
   -i      Ignore this status code e.g. -i 200 (can be used multiple times)
   -l      Location of the log file that will be emailed (default is ./status_spider.log)
   -t      Target URL
   -s      Sub process call, only used to call the child processes for any found links
   -w      Wget options
   
EOF
}


LOG_FILE='./status_spider.log'
HIST_FILE="/tmp/.spider_history"
TARGET=
SUB_PROCESS=1
IGNORE_STATUS=""
DELAY=2
MAX_THREADS=5
REFERER_URL=
VERBOSE=1
EXCLUDE=

while getopts "hd:l:t:sw:b:m:i:r:ve:" OPTION
do
     case $OPTION in
         h)
             usage
             exit 1
             ;;
         d)
             DELAY=$OPTARG
             ;;
         l)
             LOG_FILE=$OPTARG
             ;;
         t)
             TARGET=$OPTARG
             ;;
         s)
             SUB_PROCESS=0
             ;;
         w)
         	 WGET_OPTIONS=$OPTARG
         	 ;;
         b)
         	 BASE_URL=$OPTARG
         	 ;;
         m)
           MAX_THREADS=$OPTARG
           ;;
         i)
         	 IGNORE_STATUS="$IGNORE_STATUS|$OPTARG"
         	 ;;
         r)
           REFERER_URL=$OPTARG
           ;;
         v)
           VERBOSE=0
           ;;
         e)
           EXCLUDE=$OPTARG
           ;;
         ?)
             usage
             exit
             ;;
     esac
done

# tidy up ignore status
IGNORE_STATUS=`echo "$IGNORE_STATUS" | sed -r "s/^\|//"`

# default the target to http
if [[ ! "$TARGET" =~ ^[a-z]{3,5}:\/\/ ]];then
	TARGET="http://$TARGET"
fi

if [[ -z $BASE_URL ]];then
	BASE_URL=$TARGET
fi
BASE_URL=`echo "$BASE_URL" | sed -r "s|/$||"` # strip trailing slash

if [[ $SUB_PROCESS -eq 1 ]];then
	echo "" > $HIST_FILE
fi

# create a wget log file
WGET_LOG=`mktemp`
WGET_OUTPUT=`mktemp`

# exclude via regexp
if [[ "$TARGET" =~ $EXCLUDE ]];then
  exit 0;
fi

# get target
wget -O $WGET_OUTPUT --no-cache -o $WGET_LOG -k -F -B "$BASE_URL" -S $WGET_OPTIONS "$TARGET"

# log the result of the get
RESULT=`cat "$WGET_LOG" | egrep '^(  HTTP/[0-9\.]+ [0-9]+ [^\n]*|Location: [^\n]*)' | sed -r "s|  HTTP/[0-9\.]+ ([0-9]+ [^\n]*)|\1|"`
if [[ -n $RESULT && ! "$RESULT" =~ ($IGNORE_STATUS) ]];then
	echo "$TARGET $RESULT     (from: $REFERER_URL)" >> "$LOG_FILE"
fi

# exit if we are on an external link or if we don't have html
DOMAIN=`echo "$BASE_URL" | sed -r 's|^[a-z]{3,4}://([^/]+)/?.*|\1|'`
if [[ ! "$TARGET" =~ "$DOMAIN" || ! -n `pcregrep -N ANY -Mioe '(<!DOCTYPE .*?>.*?)?<html .*?>' $WGET_OUTPUT` ]];then
	exit 0;
fi

WGET_OUTPUT_FILENAME=`basename "$WGET_OUTPUT"`
# all html source on one line, filter out script blocks then find all urls filtering out the current url and urlencoding spaces
# perl is needed here for non greedy matching
LINKS=`tr "\n" ' ' < $WGET_OUTPUT | perl -pe  's#<script( [a-z]+=["'\''][^"]*["'\''])*(/>|>.*?</script>)##ig' | pcregrep -N ANY -Mioe '((?<=href=["'\''])|(?<=src=["'\'']))(.+?)(?=["'\'']( |>|/))' | grep -vF "$WGET_OUTPUT_FILENAME" | sed -r 's| |%20|ig'`

# clear up
rm $WGET_OUTPUT $WGET_LOG

for LINK in $LINKS
do
	# add on the url to any absolute urls
	LINK=`echo "$LINK" | sed -r "s|^/(.*)$|$BASE_URL/\1|"`

	# convert some entities and strip off any anchors or unneeded ampersands
	LINK=`echo "$LINK" | sed -r "s|\&amp;|\&|g" | sed -r "s|%5B|[|g" | sed -r "s|%5D|]|g" | sed -r "s/#.*$//" | sed -r "s/(&|;)+$//g"`

	# check that the link isn't blank after cleaning and that it doesn't reference the current target
	# which will be a temp file after the -k option in wget
	if [[ ! -z "$LINK" && ! "$LINK" =~ ^(mailto:|javascript:) ]];then
	
  	# default the target to http
  	if [[ ! "$LINK" =~ ^[a-z]{3,5}:\/\/ ]];then
  		LINK="http://$LINK"
  	fi
  	
		# if we haven't been there already
		if [[ ! -n `grep -F "$LINK" "$HIST_FILE"` ]];then
			COMMAND_EXTRAS=""
			if [[ ! -z $IGNORE_STATUS ]];then
				COMMAND_EXTRAS=`echo "$IGNORE_STATUS" | sed -r "s@\|?([0-9]+)\|?@-i \1@"`
			fi
			echo "$LINK" >> $HIST_FILE
			
			sleep $DELAY
      
      # spit out where we are off to in the log
      if [[ 0 -eq $VERBOSE ]];then
        echo "Checking: $LINK" >> "$LOG_FILE"
        COMMAND_EXTRAS="$COMMAND_EXTRAS -v"
      fi
      
      # if we are maxing out our thread count follow the link as a sub process
      # else spawn another instance			
			if [[ -n `ps -ea | grep status_spider | awk "NR>=$MAX_THREADS"` ]];then
			  $0 -t "$LINK" -b $BASE_URL -r "$TARGET" -l $LOG_FILE -s -w "$WGET_OPTIONS" -m $MAX_THREADS -e "$EXCLUDE" $COMMAND_EXTRAS
      else
        nohup $0 -t "$LINK" -b $BASE_URL -r "$TARGET" -l $LOG_FILE -s -w "$WGET_OPTIONS" -m $MAX_THREADS -e "$EXCLUDE" $COMMAND_EXTRAS &
      fi			
		else
      # log that we are skipping this link
      if [[ 0 -eq $VERBOSE ]];then
        echo "Skipping: $LINK" >> "$LOG_FILE"
      fi
		fi
	fi
done

exit 0
